{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcd7aace-8197-4a36-ab88-e9939c285a09",
   "metadata": {
    "id": "dcd7aace-8197-4a36-ab88-e9939c285a09"
   },
   "source": [
    "# 1. Import and install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "id": "270de208-8f06-47b6-8b0a-9131223c18d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "270de208-8f06-47b6-8b0a-9131223c18d4",
    "outputId": "4b00e389-70b0-483d-d73a-654f02d5cffe"
   },
   "source": [
    "from django.utils.lorem_ipsum import sentence\n",
    "from torch.ao.nn.quantized.functional import threshold\n",
    "!pip install -r requirements.txt -q"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5749854805bd72b8",
    "outputId": "ee07b69c-de76-4fbd-9796-490f3d95594e"
   },
   "cell_type": "code",
   "source": [
    "# Print tensorflow version\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ],
   "id": "5749854805bd72b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35a97258b3af75de",
    "outputId": "44323af8-407c-411b-b274-3afc4bab9ee1"
   },
   "cell_type": "code",
   "source": [
    "# Print mediapipe version\n",
    "import mediapipe as mp\n",
    "print(mp.__version__)"
   ],
   "id": "35a97258b3af75de",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f6c3c4290b0b3af1",
   "metadata": {
    "id": "f6c3c4290b0b3af1"
   },
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import mediapipe as mp"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "115fbd0a-dcb7-46d7-932a-343bf28743dc",
   "metadata": {
    "id": "115fbd0a-dcb7-46d7-932a-343bf28743dc"
   },
   "source": [
    "# 2. Keypoint using MP Holistic"
   ]
  },
  {
   "cell_type": "code",
   "id": "e8c25361da3f6087",
   "metadata": {
    "id": "e8c25361da3f6087"
   },
   "source": [
    "# Set up mediapipe instance\n",
    "mp_holistic = mp.solutions.holistic # Holistic models\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a9f79ad83aa97819",
   "metadata": {
    "id": "a9f79ad83aa97819"
   },
   "source": [
    "Next, I will create a function to detect by mediapipe, there are some works we need to do in this function:\n",
    "- Convert the image from BGR to RGB for detection in mediapipe\n",
    "- Set image to unwritable for saving memory\n",
    "- Make detection\n",
    "- Convert image back to BGR for rendering"
   ]
  },
  {
   "cell_type": "code",
   "id": "ea1f67aea4169609",
   "metadata": {
    "id": "ea1f67aea4169609"
   },
   "source": [
    "# Define a function to detect key points\n",
    "def mediapipe_detection(image, model):\n",
    "    image =cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # COLOR CONVERSION BGR TO RGB\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR CONVERSION RGB TO BGR\n",
    "    return image, results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5c386461-7b49-4747-bc1e-d109f775adc0",
   "metadata": {
    "id": "5c386461-7b49-4747-bc1e-d109f775adc0",
    "outputId": "5856d823-8ec8-4474-daff-ac98546018ea"
   },
   "source": [
    "# Apply the function to the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe models\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "\n",
    "        # Show to screen\n",
    "        cv2.imshow('Raw Webcam Feed', frame)\n",
    "        # Break if 'q' is pressed\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2743eabe2cf81ee4",
   "metadata": {
    "id": "2743eabe2cf81ee4",
    "outputId": "a2084574-0847-4f6e-d51d-c793fcf49aea"
   },
   "source": [
    "# Print the results\n",
    "len(results.pose_landmarks.landmark)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8627cc3f08a3ee2e",
   "metadata": {
    "id": "8627cc3f08a3ee2e"
   },
   "source": [
    "**Note**:<br>\n",
    "The face and hand landmark models will return no values if nothing is detected. The pose model will return landmarks but the visibility value inside each landmark will be low.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "9d84ce5756900deb",
   "metadata": {
    "id": "9d84ce5756900deb",
    "outputId": "8b23d729-7d3a-4a0d-d94f-04da1cfae021"
   },
   "source": [
    "# try to print other landmarks\n",
    "len(results.left_hand_landmarks.landmark)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "57a4cd7253965b23",
   "metadata": {
    "id": "57a4cd7253965b23"
   },
   "source": [
    "Now I need to visualize the landmarks on the frame. I will create a function to draw the landmarks on the frame."
   ]
  },
  {
   "cell_type": "code",
   "id": "f97794f561fb437b",
   "metadata": {
    "id": "f97794f561fb437b"
   },
   "source": [
    "# Define a function to draw landmarks\n",
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "9e86c75de658fd35",
    "outputId": "7c52c547-c431-4a25-ba1f-10584e9b73cb"
   },
   "cell_type": "code",
   "source": [
    "# The frame before applying the function\n",
    "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))"
   ],
   "id": "9e86c75de658fd35",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d82e95c712c73e56",
   "metadata": {
    "id": "d82e95c712c73e56"
   },
   "source": [
    "# Take the result from the previous cell which is the media pipe result for the frame\n",
    "results\n",
    "\n",
    "# Draw landmarks to the last frame\n",
    "draw_landmarks(frame, results)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "759d931e60f94637",
   "metadata": {
    "id": "759d931e60f94637"
   },
   "source": [
    "**Note**:<br>\n",
    "The `draw_landmarks` function does not return the image but rather applies the landmark visualizations to the current image in place."
   ]
  },
  {
   "cell_type": "code",
   "id": "fcafef96d5be183e",
   "metadata": {
    "id": "fcafef96d5be183e",
    "outputId": "28364533-f857-4068-f637-0f2b498ccf7d"
   },
   "source": [
    "# Apply the function to the frame and show to screen\n",
    "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5615b32da0c081f8",
   "metadata": {
    "id": "5615b32da0c081f8"
   },
   "source": [
    "Because our landmark for each part are the same, so I will create a function to custom style for each landmark."
   ]
  },
  {
   "cell_type": "code",
   "id": "b0fbf225d8201900",
   "metadata": {
    "id": "b0fbf225d8201900"
   },
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             )\n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    # Draw right hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "45ce3cf09c579ecb",
   "metadata": {
    "id": "45ce3cf09c579ecb"
   },
   "source": [
    "# Apply the function to the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe models\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "\n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        # Show to screen\n",
    "        cv2.imshow('Raw Webcam Feed', image)\n",
    "        # Break if 'q' is pressed\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ebd76e52b40d5889",
   "metadata": {
    "id": "ebd76e52b40d5889"
   },
   "source": [
    "# 3. Extract Keypoint Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b36c4c28d86584",
   "metadata": {
    "id": "6b36c4c28d86584"
   },
   "source": [
    "Now I will create a function to extract the keypoint values from the results and turn them into an array in numpy."
   ]
  },
  {
   "cell_type": "code",
   "id": "e31c5f36eb4a5bcf",
   "metadata": {
    "id": "e31c5f36eb4a5bcf",
    "outputId": "b684fda8-fd8e-4c93-90e8-3b499b8c07c2"
   },
   "source": [
    "len(results.pose_landmarks.landmark)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "93ca6d0f2c15f2a7",
   "metadata": {
    "id": "93ca6d0f2c15f2a7"
   },
   "source": [
    "# Get list of numpy array for each pose landmark\n",
    "pose_landmarks = []\n",
    "\n",
    "for landmark in results.pose_landmarks.landmark:\n",
    "    landmark_array = np.array([landmark.x, landmark.y, landmark.z, landmark.visibility])\n",
    "    pose_landmarks.append(landmark_array)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "67d800a69760a6ee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "id": "67d800a69760a6ee",
    "outputId": "1ac1a63e-1dd4-4402-8878-145e5205627a"
   },
   "source": [
    "# Handle the pose landmarks in a single array\n",
    "pose_landmarks = np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in results.pose_landmarks.landmark]).flatten() # Reshape to a single dimension array\n",
    "\n",
    "# Face\n",
    "if results.face_landmarks:\n",
    "    face_landmarks = np.array([[landmark.x, landmark.y, landmark.z] for landmark in results.face_landmarks.landmark]).flatten()\n",
    "else:\n",
    "    face_landmarks = np.zeros(468*3)\n",
    "\n",
    "# Left Hand\n",
    "if results.left_hand_landmarks:\n",
    "    left_hand_landmarks = np.array([[landmark.x, landmark.y, landmark.z] for landmark in results.left_hand_landmarks.landmark]).flatten()\n",
    "else:\n",
    "    left_hand_landmarks = np.zeros(21*3)\n",
    "\n",
    "# Right Hand\n",
    "if results.right_hand_landmarks:\n",
    "    right_hand_landmarks = np.array([[landmark.x, landmark.y, landmark.z] for landmark in results.right_hand_landmarks.landmark]).flatten()\n",
    "else:\n",
    "    right_hand_landmarks = np.zeros(21*3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "15137195546d3400",
   "metadata": {
    "id": "15137195546d3400"
   },
   "source": [
    "print('pose shape:', pose_landmarks.shape)\n",
    "print('face shape:', face_landmarks.shape)\n",
    "print('left hand shape:', left_hand_landmarks.shape)\n",
    "print('right hand shape:', right_hand_landmarks.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3f981d6e3fcb4f9e",
   "metadata": {
    "id": "3f981d6e3fcb4f9e",
    "outputId": "1b3bcdf5-b5a7-40b4-9ef2-1fe972d6282e"
   },
   "source": [
    "right_hand_landmarks"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bf53d0f028a8b498",
   "metadata": {
    "id": "bf53d0f028a8b498"
   },
   "source": [
    "def extract_keypoints(results):\n",
    "    # Pose\n",
    "    if results.pose_landmarks:\n",
    "        pose_landmarks = np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in results.pose_landmarks.landmark]).flatten() # Reshape to a single dimension array\n",
    "    else:\n",
    "        pose_landmarks = np.zeros(33*4)\n",
    "\n",
    "    # Face\n",
    "    if results.face_landmarks:\n",
    "        face_landmarks = np.array([[landmark.x, landmark.y, landmark.z] for landmark in results.face_landmarks.landmark]).flatten()\n",
    "    else:\n",
    "        face_landmarks = np.zeros(468*3)\n",
    "\n",
    "    # Left Hand\n",
    "    if results.left_hand_landmarks:\n",
    "        left_hand_landmarks = np.array([[landmark.x, landmark.y, landmark.z] for landmark in results.left_hand_landmarks.landmark]).flatten()\n",
    "    else:\n",
    "        left_hand_landmarks = np.zeros(21*3)\n",
    "\n",
    "    # Right Hand\n",
    "    if results.right_hand_landmarks:\n",
    "        right_hand_landmarks = np.array([[landmark.x, landmark.y, landmark.z] for landmark in results.right_hand_landmarks.landmark]).flatten()\n",
    "    else:\n",
    "        right_hand_landmarks = np.zeros(21*3)\n",
    "\n",
    "    return np.concatenate([pose_landmarks, face_landmarks, left_hand_landmarks, right_hand_landmarks])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fd9e8396735ba36c",
   "metadata": {
    "id": "fd9e8396735ba36c",
    "outputId": "9676ba70-482d-4f51-edb0-8c383eb0ecf1"
   },
   "source": [
    "# Test the function\n",
    "print('First 10 result: ', extract_keypoints(results)[:10])\n",
    "print('shape: ', extract_keypoints(results).shape)\n",
    "print('shape = 33*4 + 468*3 + 21*3 + 21*3 = ', 33*4 + 468*3 + 21*3 + 21*3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d95caf2d6013e771",
   "metadata": {
    "id": "d95caf2d6013e771"
   },
   "source": [
    "# 4. Setup Folders for Collection"
   ]
  },
  {
   "cell_type": "code",
   "id": "12b2ae1b14728c6a",
   "metadata": {
    "id": "12b2ae1b14728c6a"
   },
   "source": [
    "# Define paths for exported data, nnumpy array\n",
    "DATA_PATH = '/input/processed/MP_Data'\n",
    "\n",
    "# Actions that I try to detect\n",
    "actions = np.array(['hello', 'thanks', 'iloveyou'])\n",
    "\n",
    "# Number videos worth of data\n",
    "no_sequences = 30\n",
    "\n",
    "# Number of frames for each video\n",
    "sequence_length = 30"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "194aaea28df932ad",
   "metadata": {
    "id": "194aaea28df932ad"
   },
   "source": [
    "The main different between **action detection** and other computer vision tasks is that a sequences of data than a single frame is used for detection."
   ]
  },
  {
   "cell_type": "code",
   "id": "7b07cb9b2e0fdb7f",
   "metadata": {
    "id": "7b07cb9b2e0fdb7f"
   },
   "source": [
    "# Create a folder for each action\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a60b5ccc62e8d39b",
   "metadata": {
    "id": "a60b5ccc62e8d39b"
   },
   "source": [
    "The folder tree will look like this:\n",
    "\n",
    "```bash\n",
    "MP_Data\n",
    "|__ a\n",
    "|   |____ 0\n",
    "|   |____ 1\n",
    "|   |____ ...\n",
    "|__ b\n",
    "|   |____ 0\n",
    "|   |____ 1\n",
    "|   |____ ...\n",
    "|__ c\n",
    "|   |____ 0\n",
    "|   |____ 1\n",
    "|   |____ ...\n",
    "|__ hello\n",
    "|   |____ 0\n",
    "|   |____ 1\n",
    "|   |____ ...\n",
    "|__ thanks\n",
    "|   |____ 0\n",
    "|   |____ 1\n",
    "|   |____ ...\n",
    "|__ iloveyou\n",
    "|   |____ 0\n",
    "|   |____ 1\n",
    "|   |____ ...\n",
    "```\n",
    "<br>\n",
    "I am going to collect 30 videos per action. Then each one of those video sequences will contain 30 frames of data. Each frame will contain 1662 landmark values. 30*3 sequences, 30 frames, 1662 landmarks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25df693989d7ff9",
   "metadata": {
    "id": "a25df693989d7ff9"
   },
   "source": [
    "# 5. Collect Keypoint Values for Training and Testing\n",
    "In this section, I will collect the keypoint values for training and testing. I will use the `cv2.putText` function to display the action and sequence number on the screen. I will also use the `cv2.waitKey` function to pause the screen for 2 seconds before starting to collect the data. <br>\n",
    "For saving extracted data, I will use the `np.save` function to save the data.<br>\n",
    "For loading the data, I will use the `np.load` function to load the data."
   ]
  },
  {
   "cell_type": "code",
   "id": "304db0f21ff9f831",
   "metadata": {
    "id": "304db0f21ff9f831",
    "outputId": "54573cf1-e337-4314-92ea-086a33359968"
   },
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe models\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "\n",
    "    # NEW LOOP\n",
    "    # Loop through actions\n",
    "    for action in actions:\n",
    "        # Loop through sequences aka videos\n",
    "        for sequence in range(no_sequences):\n",
    "            # Loop through video length aka sequence length\n",
    "            for frame_num in range(sequence_length):\n",
    "\n",
    "                # Read feed\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                # Make detections\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "#                 print(results)\n",
    "\n",
    "                # Draw landmarks\n",
    "                draw_styled_landmarks(image, results)\n",
    "\n",
    "                # NEW Apply wait logic\n",
    "                if frame_num == 0:\n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120,200),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    cv2.waitKey(3000)\n",
    "                else:\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "                # NEW Export keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                # Break gracefully\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d4049640943eb644",
   "metadata": {
    "id": "d4049640943eb644"
   },
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d608797c668857aa",
   "metadata": {
    "id": "d608797c668857aa"
   },
   "source": [
    "# 6. Preprocess Data and Create Labels and Features"
   ]
  },
  {
   "cell_type": "code",
   "id": "8098192ad1845263",
   "metadata": {
    "id": "8098192ad1845263"
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c763f0261f018337",
   "metadata": {
    "id": "c763f0261f018337"
   },
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c79f41160d959afd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c79f41160d959afd",
    "outputId": "f09a3415-2dbf-427b-bbe5-66eba1f1bf13"
   },
   "source": [
    "label_map"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9b06ec5dba942245",
   "metadata": {
    "id": "9b06ec5dba942245"
   },
   "source": [
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        window = [] # All frames in a single sequence\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num))) # Load frame\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "633c3443369281ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "633c3443369281ca",
    "outputId": "aa52f5f9-d322-4c0f-a241-77894d5233dd"
   },
   "source": [
    "np.array(sequences).shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7f2f29707601c48e",
   "metadata": {
    "id": "7f2f29707601c48e"
   },
   "source": [
    "The shape of the sequences is (180, 30, 1662). This means that there are 180 sequences, each with 30 frames and 1662 landmarks. <br>"
   ]
  },
  {
   "cell_type": "code",
   "id": "5930acddbe96a950",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5930acddbe96a950",
    "outputId": "62c147ca-afcb-49b6-9cbf-7c6aa58399a5"
   },
   "source": [
    "np.array(labels).shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ac64fe33a2fdeaf8",
   "metadata": {
    "id": "ac64fe33a2fdeaf8"
   },
   "source": [
    "The shape of the labels is (180,). This means that there are 180 labels, one for each sequence."
   ]
  },
  {
   "cell_type": "code",
   "id": "e1bccb5e750ab38a",
   "metadata": {
    "id": "e1bccb5e750ab38a"
   },
   "source": [
    "X = np.array(sequences)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5776ab5df033ec25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5776ab5df033ec25",
    "outputId": "e5dc98d5-c5bc-44f2-e032-0190c6cd191d"
   },
   "source": [
    "X.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5aea858f74950461",
   "metadata": {
    "id": "5aea858f74950461"
   },
   "source": [
    "y = to_categorical(labels).astype(int)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "543e34736d7352ac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "543e34736d7352ac",
    "outputId": "9807356a-530c-4035-d686-e9e88d61f235"
   },
   "source": [
    "y"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c82053c8a4181b8d",
   "metadata": {
    "id": "c82053c8a4181b8d"
   },
   "source": [
    "Next, I am going to create the training and testing sets using the `train_test_split` function from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "id": "6840d63415ddca82",
   "metadata": {
    "id": "6840d63415ddca82"
   },
   "source": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "de395437b2c0984f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "de395437b2c0984f",
    "outputId": "db9bec12-3dd0-4b71-d6d9-42b42ea80457"
   },
   "source": [
    "# Print the shape of the training and testing sets\n",
    "X_train.shape, X_test.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This mean our model is processing sequential data for sign language recognition.\n",
    "Each input sample consists of a sequence of 30 frames, where each frame contains 1662 features (which is the keypoints of hand tracking get from mediapipe).\n",
    "We have 81 training samples and 9 testing samples."
   ],
   "id": "c3ca9a5114b4a269"
  },
  {
   "cell_type": "code",
   "id": "c517fc794271a2ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c517fc794271a2ed",
    "outputId": "f9d4b6e8-2a76-4646-accf-46f8fdbf515a"
   },
   "source": [
    "y_train.shape, y_test.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5f04b83c3550f247",
   "metadata": {
    "id": "5f04b83c3550f247"
   },
   "source": [
    "# 7. Build and Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "id": "e02f63b9333c490c",
   "metadata": {
    "id": "e02f63b9333c490c"
   },
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6bc4434d9517f80b",
   "metadata": {
    "id": "6bc4434d9517f80b"
   },
   "source": [
    "Next, I am going to create a log directory and set up for tensorBoard callback."
   ]
  },
  {
   "cell_type": "code",
   "id": "73d5ad95a2a5c6f",
   "metadata": {
    "id": "73d5ad95a2a5c6f"
   },
   "source": [
    "# TensorBoard logging\n",
    "log_dir = os.path.join('../log')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "befb9f822d180f0b"
   },
   "cell_type": "code",
   "source": [
    "# Early stopping to prevent overfitting\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)"
   ],
   "id": "befb9f822d180f0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "91ab1de8ff8d49c0",
   "metadata": {
    "id": "91ab1de8ff8d49c0"
   },
   "source": [
    "# Model Definition\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, activation='tanh', input_shape=(30, 1662)),\n",
    "    LSTM(128, return_sequences=True, activation='tanh'),\n",
    "    LSTM(64, return_sequences=False, activation='tanh'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(actions), activation='softmax')\n",
    "])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "be8ae87b717140aa"
   },
   "cell_type": "code",
   "source": [
    "# Compile Model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])"
   ],
   "id": "be8ae87b717140aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b7be7d24d90ae40",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b7be7d24d90ae40",
    "outputId": "eba496a5-4ceb-49c9-e59e-65ff7bb3d468"
   },
   "source": [
    "# Train Model\n",
    "history = model.fit(X_train, y_train,\n",
    "          validation_data=(X_test, y_test),\n",
    "          epochs=1000,\n",
    "          batch_size=32,\n",
    "          callbacks=[tb_callback, early_stop])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, the model have achieved a pretty high accuracy"
   ],
   "metadata": {
    "id": "dCoPkIh-qQeF"
   },
   "id": "dCoPkIh-qQeF"
  },
  {
   "cell_type": "code",
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ksYToW_YqH05",
    "outputId": "b9483d98-0cbb-4577-bc43-1baf6454a15e"
   },
   "id": "ksYToW_YqH05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next, I will visualize the training progress.<br>\n",
    "I will create a function to smooth the curves by reducing fluctuations in the values across epochs.\n",
    "This helps create a clearer trend by averaging out sharp variations in loss and accuracy."
   ],
   "id": "98f860d8ca328f2a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# NEW FUNC: Apply smoothing (moving average)\n",
    "def smooth_curve(points, factor=0.8):\n",
    "    smoothed_points = []\n",
    "    for i, point in enumerate(points):\n",
    "        if i == 0:\n",
    "            smoothed_points.append(point)\n",
    "        else:\n",
    "            smoothed_points.append(smoothed_points[-1] * factor + point * (1 - factor))\n",
    "    return smoothed_points"
   ],
   "id": "52e4c2f000a6b43c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extract training data for visualize\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_acc = history.history['categorical_accuracy']\n",
    "val_acc = history.history['val_categorical_accuracy']\n",
    "\n",
    "# Start counting epoch from 1\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "# Smooth\n",
    "train_loss_smooth = smooth_curve(train_loss)\n",
    "val_loss_smooth = smooth_curve(val_loss)\n",
    "train_acc_smooth = smooth_curve(train_acc)\n",
    "val_acc_smooth = smooth_curve(val_acc)\n"
   ],
   "id": "50f151e04391f0cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Limit epochs shown\n",
    "max_epochs = min(200, len(epochs))  # Show only first 200 epochs\n",
    "epochs = epochs[:max_epochs]\n",
    "\n",
    "# Plot Loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_loss_smooth[:max_epochs], 'b', label='Training Loss')\n",
    "plt.plot(epochs, val_loss_smooth[:max_epochs], 'r', label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_acc_smooth[:max_epochs], 'b', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc_smooth[:max_epochs], 'r', label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training vs Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n"
   ],
   "id": "10842b76235dade0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 8. Make Predictions\n"
   ],
   "metadata": {
    "id": "cMvInCM2qjHO"
   },
   "id": "cMvInCM2qjHO"
  },
  {
   "cell_type": "code",
   "source": "res = model.predict(X_test)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9P7B4RwnqlJD",
    "outputId": "88e3af74-c199-4f77-c156-ce4e5bcaa032"
   },
   "id": "9P7B4RwnqlJD",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(res)",
   "id": "c44f7a2023b5092d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(\"Class Probabilities for Sample 3:\", res[3])",
   "id": "db354fd9cde589ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "actions[np.argmax(res[3])]",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "GH_cH0bPqned",
    "outputId": "48a297b6-fe01-4084-ee3c-931aa26e3962"
   },
   "id": "GH_cH0bPqned",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "actions[np.argmax(y_test[3])]\n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "sUTzf_vXqtrc",
    "outputId": "d84db9fe-b66f-40e1-da10-f8c7addb2104"
   },
   "id": "sUTzf_vXqtrc",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 9. Save Weights"
   ],
   "metadata": {
    "id": "UDyh1SuAq4sv"
   },
   "id": "UDyh1SuAq4sv"
  },
  {
   "cell_type": "code",
   "source": "model.save('/home/martinvalentine/Desktop/SignLanguageDetectionLSTM/exp_v1_no_dropout-0.0005/models.h5')",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Mk5LzqSqxZ_",
    "outputId": "6179a47a-6703-48e8-ce63-ae5d2c3c7b14"
   },
   "id": "1Mk5LzqSqxZ_",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "del model"
   ],
   "metadata": {
    "id": "E2z-PcaFrEfX"
   },
   "id": "E2z-PcaFrEfX",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# models.load_weights('/content/sign_language.h5')\n",
    "model.load_weights('/home/martinvalentine/Desktop/SignLanguageDetectionLSTM/exp_v1_no_dropout-0.0005/models.h5')"
   ],
   "metadata": {
    "id": "TQl1-dasrS7e"
   },
   "id": "TQl1-dasrS7e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 10. Evaluation using Confusion Matrix and Accuracy"
   ],
   "metadata": {
    "id": "b2_qp3n5r3HD"
   },
   "id": "b2_qp3n5r3HD"
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ],
   "metadata": {
    "id": "i8u8WYEhr_kr"
   },
   "id": "i8u8WYEhr_kr",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "yhat = model.predict(X_test)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HRm-Yw_RsCcx",
    "outputId": "a6e66233-1c06-461c-ef4e-02cf666933d6"
   },
   "id": "HRm-Yw_RsCcx",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ],
   "metadata": {
    "id": "FNPK8mEisFK7"
   },
   "id": "FNPK8mEisFK7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The **Confusion Matrix** is organised as follows:<br>\n",
    ">[[TRUE N, FALSE N],<br>\n",
    ">[FALSE N, TRUE P]]"
   ],
   "id": "954ecc5864b26177"
  },
  {
   "cell_type": "code",
   "source": [
    "cm = multilabel_confusion_matrix(ytrue, yhat)\n",
    "print(cm)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9tUxNnVOsI7s",
    "outputId": "e7c9a616-86ae-4967-c51b-c370a433782b"
   },
   "id": "9tUxNnVOsI7s",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualized plot\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot each class confusion matrix\n",
    "num_classes = len(cm)\n",
    "fig, axes = plt.subplots(1, num_classes, figsize=(4 * num_classes, 4))\n",
    "\n",
    "for i in range(num_classes):\n",
    "    sns.heatmap(cm[i], annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[i])\n",
    "    axes[i].set_title(f\"Class {i}\")\n",
    "    axes[i].set_xlabel(\"Predicted\")\n",
    "    axes[i].set_ylabel(\"True\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "16bf53e7f8f103c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "If you're still unsure about how to interpret a **confusion matrix**, here's a breakdown of what each value means:\n",
    "\n",
    "---\n",
    "\n",
    "| **True \\ Predicted** | **Predicted 0** | **Predicted 1** |\n",
    "|----------------------|----------------|----------------|\n",
    "| **True 0** (Negative Class) | **TN** (True Negative) | **FP** (False Positive) |\n",
    "| **True 1** (Positive Class) | **FN** (False Negative) | **TP** (True Positive) |\n",
    "\n",
    "**What Each Term Means:**\n",
    "- **TN (Top-Left Corner):** Correctly predicted **0** (Negative class).\n",
    "- **FP (Top-Right Corner):** Incorrectly predicted **1** (but it was actually **0**).\n",
    "- **FN (Bottom-Left Corner):** Incorrectly predicted **0** (but it was actually **1**).\n",
    "- **TP (Bottom-Right Corner):** Correctly predicted **1** (Positive class).\n",
    "\n",
    "---\n",
    "\n",
    "#### Example: Confusion Matrix for **Class 0**\n",
    "From the **first plot (Class 0):**\n",
    "- **TN = 5** (Correctly predicted **\"not class 0\"**)\n",
    "- **FP = 0** (Did not wrongly predict class 0)\n",
    "- **FN = 0** (Did not wrongly ignore class 0)\n",
    "- **TP = 4** (Correctly predicted **class 0**)\n",
    "\n",
    "**➡ Model performance for Class 0:**\n",
    "✔ **4 correct predictions and 0 mistakes!**\n",
    "\n",
    "---\n",
    "\n",
    "#### What we can do with this plot?\n",
    "- If a class has **low TP (True Positives)**, consider **balancing the dataset**.\n",
    "- If FP or FN are high, the model may need **hyperparameter tuning** or **better features**.\n",
    "- Use this breakdown to **evaluate how well your model performs per class**.\n"
   ],
   "id": "16d6a7e7fad1ed7b"
  },
  {
   "cell_type": "code",
   "source": [
    "accuracy_score(ytrue, yhat)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Hp268s1sMzM",
    "outputId": "4d41bc95-702b-4e61-97b7-d48ef3bc5a05"
   },
   "id": "7Hp268s1sMzM",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 11. Test in Real Time"
   ],
   "metadata": {
    "id": "bmjFeBIGsPbM"
   },
   "id": "bmjFeBIGsPbM"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "27d8b4fabd906c01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pred_res = model.predict(np.expand_dims(X_test[0], axis=0))[0]",
   "id": "ee1f849f709f9a0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pred_res[np.argmax(pred_res)]",
   "id": "d93c26ae92ac2392",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. NEW detection variables\n",
    "sequence = [] # For storing 30 frames in order to make a prediction on\n",
    "sentence = [] # Concatenate detections history\n",
    "threshold = 0.8\n",
    "\n",
    "# Apply the function to the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe models\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "\n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        # 2. PREDICTION LOGIC\n",
    "        keypoints = extract_keypoints(results)\n",
    "        # sequence.append(keypoints)\n",
    "        sequence.insert(0,keypoints)\n",
    "        # sequence = sequence[-30:] # Grab the last 30 frames to make prediction\n",
    "        sequence = sequence[:30]\n",
    "\n",
    "        if len(sequence) == 30:\n",
    "            input_sequence =  np.expand_dims(sequence, axis=0) # Explain in bellow\n",
    "            print(\"Model input shape:\", input_sequence.shape)  # (1, 30, 1662)\n",
    "\n",
    "            # Predict sign language action\n",
    "            res = model.predict(input_sequence)\n",
    "\n",
    "            # Reshape output if necessary\n",
    "            res = res[0]  # Extract first batch prediction\n",
    "            res = res.flatten()  # Ensure it is a 1D array\n",
    "\n",
    "            # Determine the most likely action\n",
    "            predicted_action = actions[np.argmax(res)]\n",
    "            print(\"Predicted action:\", predicted_action)\n",
    "\n",
    "        # 3. VISUALIZATION LOGIC\n",
    "            if res[np.argmax(res)] > threshold:\n",
    "                if len(sentence) > 0:\n",
    "                    if actions[np.argmax(res)] != sentence[-1]:  # Avoid consecutive duplicates\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "                else:\n",
    "                    sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "                if len(sentence) > 5:\n",
    "                    sentence = sentence[-5:]  # Keep only the last 5 words\n",
    "\n",
    "        cv2.rectangle(image, (0,0), (640,40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Show to screen\n",
    "        cv2.imshow('Raw Webcam Feed', image)\n",
    "\n",
    "        # Break if 'q' is pressed\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ],
   "id": "3a8f84f322e0b7fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### What is `np.expandims()` function?\n",
    "If we run this bellow code to make prediction on a `X_test` item:\n",
    "```python\n",
    "model.predict(X_test[0])\n",
    "```\n",
    "We will get an error because the shape of this item is incorrect.\n"
   ],
   "id": "2104293524c11167"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.predict(X_test[0])",
   "id": "c98737b794be62b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Print the shape of X_test[0]\n",
    "X_test[0].shape"
   ],
   "id": "fb33fbfaad712b21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The shape of this item is `(30, 1662)` but the shape that our model expected is `(num_sequences, 30, 1662)` with `num_sequences` equal to 30 (30 frames). So in this case we will need to expand dimension of this item by using `np.expandims()` function:\n",
    "```python\n",
    "import numpy as np\n",
    "np.expand_dims(X_test[0], axis=0)\n",
    "```\n",
    "with `axis=0` meaning we are adding a new dimension at the 0th axis (batch dimension).\n",
    "\n",
    "This transforms the shape from **(30, 1662) → (1, 30, 1662)**, making it compatible with the model's expected input format."
   ],
   "id": "8fe9dbc8d7d67999"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "np.expand_dims(X_test[0], axis=0)"
   ],
   "id": "1f52b1ebe9b77c09",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "np.expand_dims(X_test[0], axis=0).shape",
   "id": "e7035c6dac306865",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we can test with our model.",
   "id": "25b283006bb8bc79"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.predict(np.expand_dims(X_test[0], axis=0))",
   "id": "e849013da8ee3930",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "actions[np.argmax(model.predict(np.expand_dims(X_test[0], axis=0)))]",
   "id": "a6011b674692c227",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "actions[np.argmax(y_test[0])]",
   "id": "49c988ff102b94d6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
